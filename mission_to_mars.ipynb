{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1\n",
    "#soup set up\n",
    "\n",
    "url1 = 'https://mars.nasa.gov/news/'\n",
    "html = requests.get(url1).text\n",
    "soup = bs(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA Ingenuity Mars Helicopter Prepares for First Flight\n"
     ]
    }
   ],
   "source": [
    "#first news story headline variable storage\n",
    "\n",
    "newstitle = soup.find(class_ = \"content_title\" ).text.strip()\n",
    "print(newstitle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now uncocooned from its protective carbon-fiber shield, the helicopter is being readied for its next steps.\n"
     ]
    }
   ],
   "source": [
    "#first news story description variable storage\n",
    "newsdescription = soup.find(class_ = 'rollover_description_inner')\n",
    "newsdescription = newsdescription.text.strip()\n",
    "print(newsdescription)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\Brian Morton.000\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "ename": "ElementDoesNotExist",
     "evalue": "no elements could be found with link by partial text \" FULL IMAGE\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c3781973a7f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by_partial_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' FULL IMAGE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m#alt syntax?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#browser.find_link_by_text('FULL IMAGE').first.click()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0melement_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\splinter\\element_list.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     44\u001b[0m             raise ElementDoesNotExist(\n\u001b[0;32m     45\u001b[0m                 u'no elements could be found with {0} \"{1}\"'.format(\n\u001b[1;32m---> 46\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_by\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 )\n\u001b[0;32m     48\u001b[0m             )\n",
      "\u001b[1;31mElementDoesNotExist\u001b[0m: no elements could be found with link by partial text \" FULL IMAGE\""
     ]
    }
   ],
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "\n",
    "#* Visit the url for JPL Featured Space Image [here](https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html).\n",
    "url2 = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "#* Use splinter to navigate the site and find the image url for the current Featured Mars Image\n",
    "#and assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "# Setup splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "browser.visit(url2)\n",
    "\n",
    "\n",
    "\n",
    "browser.links.find_by_partial_text(' FULL IMAGE').click()\n",
    "#alt syntax?\n",
    "#browser.find_link_by_text('FULL IMAGE').first.click()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "html = requests.get(url2).text\n",
    "soup2 = bs(html, 'html.parser')\n",
    "\n",
    "featured_image_url = soup2.find(class_ = \"header\" )\n",
    "featured_image_url\n",
    "\n",
    "\n",
    "#hardcoded url?\n",
    "featured_image_url = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/image/featured/mars2.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mars Facts\n",
    "url3 = 'https://space-facts.com/mars/'\n",
    "# Visit the Mars Facts webpage [here](https://space-facts.com/mars/) and use Pandas to scrape the table \n",
    "#containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "read = pd.read_html(url3)\n",
    "dftable= tables[1]\n",
    "\n",
    "dftable = dftable.drop(['Earth'], axis=1)\n",
    "dftable\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "html_table = dftable.to_html()\n",
    "html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus Hemisphere Enhanced\n",
      "Schiaparelli Hemisphere Enhanced\n",
      "Syrtis Major Hemisphere Enhanced\n",
      "Valles Marineris Hemisphere Enhanced\n"
     ]
    }
   ],
   "source": [
    "### Mars Hemispheres\n",
    "\n",
    "#* Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) \n",
    "#to obtain high resolution images for each of Mar's hemispheres.\n",
    "url4 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "html = requests.get(url4).text\n",
    "soup3 = bs(html, 'html.parser')\n",
    "      \n",
    "    \n",
    "    \n",
    "hemindex = soup3.find_all('h3')\n",
    "\n",
    "Cerebus_title = hemindex[0].text.strip()\n",
    "print(Cerebus_title)\n",
    "\n",
    "\n",
    "Schiaparelli_title = hemindex[1].text.strip()\n",
    "print (Schiaparelli_title)\n",
    "\n",
    "\n",
    "Syrtis_Major_title = hemindex[2].text.strip()\n",
    "print (Syrtis_Major_title)\n",
    "\n",
    "\n",
    "Valles_Marineris_title = hemindex[3].text.strip()\n",
    "print (Valles_Marineris_title)\n",
    "\n",
    "\n",
    "\n",
    "#hardcoded urls\n",
    "Cerebus_url = 'https://astrogeology.usgs.gov/cache/images/39d3266553462198bd2fbc4d18fbed17_cerberus_enhanced.tif_thumb.png'\n",
    "Schiaparelli_url = 'https://astrogeology.usgs.gov/cache/images/08eac6e22c07fb1fe72223a79252de20_schiaparelli_enhanced.tif_thumb.png'\n",
    "Syrtis_url = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced'\n",
    "Valles_url = 'https://astrogeology.usgs.gov/cache/images/4e59980c1c57f89c680c0e1ccabbeff1_valles_marineris_enhanced.tif_thumb.png'\n",
    "\n",
    "\n",
    "#* Save both the image url string for the full resolution hemisphere image, and the Hemisphere title \n",
    "#containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "\n",
    "#* Append the dictionary with the image url string and the hemisphere title to a list. \n",
    "#This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "hemisphere_image_urls = []\n",
    "hemisphere_image_urls.append({'\"title\"':Cerebus_title, 'img_url':Cerebus_url})\n",
    "hemisphere_image_urls.append({'\"title\"':Schiaparelli_title, 'img_url':Schiaparelli_url})\n",
    "hemisphere_image_urls.append({'\"title\"':Syrtis_Major_title, 'img_url':Syrtis_url})\n",
    "hemisphere_image_urls.append({'\"title\"':Valles_Marineris_title, 'img_url':Valles_url})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scraping link fix section 2? only returning hashes\n",
    "response = requests.get(url2)\n",
    " \n",
    "# Extracting the source code of the page.\n",
    "data = response.text\n",
    " \n",
    "# Passing the source code to BeautifulSoup to create a BeautifulSoup object for it.\n",
    "souptest = bs(data, 'lxml')\n",
    " \n",
    "# Extracting all the <a> tags into a list.\n",
    "tags = souptest.find_all('a')\n",
    " \n",
    "# Extracting URLs from the attribute href in the <a> tags.\n",
    "for tag in tags:\n",
    "    print(tag.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.usgs.gov/centers/astrogeo-sc\n",
      "https://nasa.gov\n",
      "https://www.usgs.gov/centers/astrogeology-science-center/science/pds-cartography-and-imaging-sciences-node-usgs\n",
      "/search\n",
      "/search/map/Mars/Viking/cerberus_enhanced\n",
      "/search/map/Mars/Viking/schiaparelli_enhanced\n",
      "/search/map/Mars/Viking/syrtis_major_enhanced\n",
      "/search/map/Mars/Viking/valles_marineris_enhanced\n",
      "http://isis.astrogeology.usgs.gov\n",
      "http://planetarynames.wr.usgs.gov\n",
      "https://astrogeology.usgs.gov/tools/map-a-planet-2\n",
      "https://www.usgs.gov/centers/astrogeo-sc/science/cartography-and-imaging-sciences-node-nasa-planetary-data-system\n",
      "https://www.usgs.gov/centers/astrogeo-sc/science/regional-planetary-image-facility-rpif\n",
      "\n",
      "http://pilot.wr.usgs.gov\n",
      "https://www.usgs.gov/centers/astrogeo-sc/science/mrctr-gis-lab\n",
      "./search\n",
      "http://astrogeology.usgs.gov/maps/about\n",
      "http://astrogeology.usgs.gov/maps/contact\n",
      "https://www.usgs.gov/centers/astrogeo-sc\n"
     ]
    }
   ],
   "source": [
    "# scraping link fix section 4? only returning hashes\n",
    "response = requests.get(url4)\n",
    " \n",
    "# Extracting the source code of the page.\n",
    "data = response.text\n",
    " \n",
    "# Passing the source code to BeautifulSoup to create a BeautifulSoup object for it.\n",
    "souptest = bs(data, 'lxml')\n",
    " \n",
    "# Extracting all the <a> tags into a list.\n",
    "tags = souptest.find_all('a')\n",
    " \n",
    "# Extracting URLs from the attribute href in the <a> tags.\n",
    "for tag in tags:\n",
    "    print(tag.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
